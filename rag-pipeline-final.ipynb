{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418400d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53bcfb8",
   "metadata": {},
   "source": [
    "## Extract texts, tables, images using Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "output_path = \"./\"\n",
    "file_path = output_path + \"data/book.pdf\"\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,\n",
    "\n",
    "    strategy=\"hi_res\",\n",
    "    languages=['ben', 'eng'],\n",
    "\n",
    "    extract_image_block_types=['Image', 'Table'],\n",
    "    extract_image_block_to_payload=True,\n",
    "\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters = 10000,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    new_after_n_chars=6000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73627ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts from the chunks\n",
    "\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if 'CompositeElement' in str(type(chunk)):\n",
    "        texts.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d9b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images from CompositeElements\n",
    "\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if 'CompositeElement' in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if 'Image' in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    \n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55d9d2",
   "metadata": {},
   "source": [
    "## Summarize the content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01faea4",
   "metadata": {},
   "source": [
    "First we will chunk the extracted texts. Then we will create summary of text chunks and images and then vectorize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdad871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd01c1",
   "metadata": {},
   "source": [
    "**N.B:** Llama-4-maverick and Llama-4-scout both are tested. Llama-4-maverick provides better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e604272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_maverick = ChatGroq(\n",
    "    temperature = 0.5,\n",
    "    model = \"meta-llama/llama-4-maverick-17b-128e-instruct\"  \n",
    ")\n",
    "\n",
    "llm_scout = ChatGroq(\n",
    "    temperature = 0.5,\n",
    "    model = \"meta-llama/llama-4-scout-17b-16e-instruct\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a4a7c",
   "metadata": {},
   "source": [
    "### Text Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf4c60",
   "metadata": {},
   "source": [
    "**Text Chunking Strategy:**\n",
    "\n",
    "1. Each chunk in texts is a Composite Element.\n",
    "2. We will extract the texts from each chunk first.\n",
    "3. Then those extracted text will be passed to RecursiveCharacterTextSplitter to split texts in a token aware setup.\n",
    "4. Token aware strategy is followed because, Jina Embeddings model has a token limit of 8196.\n",
    "5. Tiktoken is used to count token of each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9493fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(texts):\n",
    "    \"\"\"Extracts text from a list of CompositeElements or text objects.\"\"\"\n",
    "    texts_only = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, dict):\n",
    "            if 'text' in text:\n",
    "                texts_only.append(text['text'])\n",
    "        elif hasattr(text, 'text'):\n",
    "            texts_only.append(text.text)\n",
    "    \n",
    "    return \"\\n\".join(texts_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tiktoken\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def token_length(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=7000,               # Max tokens per chunk\n",
    "    chunk_overlap=200,             # Optional overlap for context continuity\n",
    "    length_function=token_length,  # Token-aware length function\n",
    "    separators=[\n",
    "        \"\\n\\n\",                    # Paragraph breaks\n",
    "        \"\\n\",                      # Line breaks\n",
    "        \"।\",                       # Bangla sentence ending\n",
    "        \".\",                       # English sentence ending\n",
    "        \"!\",                       # Exclamation\n",
    "        \"?\",                       # Question\n",
    "        \";\",                       # Semicolon\n",
    "        \" \",                       # Word level\n",
    "        \"\"                         # Character level (last resort)\n",
    "    ],\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "chunked_texts = text_splitter.split_text(get_texts(texts))\n",
    "\n",
    "chunked_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335bfb52",
   "metadata": {},
   "source": [
    "### Text and tables summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831f064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "\n",
    "prompt_text_summary = \"\"\"\n",
    "তুমি একজন বুদ্ধিমান ভাষা সহকারী, যার উদ্দেশ্য হলো একটি শিক্ষামূলক পাঠ্যবইয়ের নির্দিষ্ট অংশ (টেক্সট বা টেবিল) থেকে উচ্চ-গুণমানের, তথ্যসমৃদ্ধ সারাংশ তৈরি করা যা পরবর্তী পর্যায়ে প্রশ্নোত্তর ভিত্তিক ব্যবস্থায় কার্যকরভাবে ব্যবহৃত হতে পারে।\n",
    "\n",
    "তোমার সারাংশ হতে হবে:\n",
    "- মূল বিষয়বস্তু ও তথ্য সুনির্দিষ্টভাবে ধারণকারী  \n",
    "- প্রসঙ্গ বজায় রেখে তৈরি   \n",
    "- সংক্ষিপ্ত কিন্তু পরিপূর্ণ  \n",
    "\n",
    "তুমি যেটা *করবে না*:\n",
    "- ভূমিকা, উপসংহার, বা ব্যক্তিগত ব্যাখ্যা দেবে না  \n",
    "- \"সারাংশ:\", \"এই অংশে বলা হয়েছে\" বা অনুরূপ বাক্যাংশ দিয়ে শুরু করবে না  \n",
    "- অতিরিক্ত ব্যাখ্যামূলক বা অলংকারপূর্ণ ভাষা ব্যবহার করবে না  \n",
    "\n",
    "উদ্দেশ্য হলো: পাঠ্যবস্তুর জ্ঞানগত কাঠামো সংরক্ষণ করে এমন একটি embedding-উপযোগী সারাংশ তৈরি করা।\n",
    "\n",
    "এখন নিচের অংশ থেকে একটি সারাংশ তৈরি করো: {element}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_text_summary)\n",
    "\n",
    "text_summary_chain = prompt | llm_maverick | StrOutputParser()\n",
    "\n",
    "text_summaries = text_summary_chain.batch(chunked_texts, {\"max_concurrency\": 1})\n",
    "\n",
    "text_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e4820",
   "metadata": {},
   "source": [
    "### Image summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd92ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_image_summary = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": \"\"\"\n",
    "                তুমি একজন দক্ষ চিত্র বিশ্লেষক ভাষা সহকারী, যার কাজ হলো একটি চিত্র (যেমন টেবিল, ডায়াগ্রাম, চিত্রিত উদাহরণ, বা সংক্ষিপ্ত লেখা-সহ চিত্র) দেখে একটি সংক্ষিপ্ত, তথ্যবহুল সারাংশ তৈরি করা।\n",
    "\n",
    "                তোমার সারাংশ:\n",
    "                - চিত্রের মূল তথ্য ও কাঠামো ধরে রাখবে\n",
    "                - ব্যাখ্যামূলক এবং তথ্যসমৃদ্ধ হবে\n",
    "                - ভবিষ্যৎ প্রশ্নোত্তর ব্যবস্থায় ব্যবহারের উপযোগী হবে\n",
    "\n",
    "                তুমি যেটা *করবে না*:\n",
    "                - চিত্রটি কী তা পুনরাবৃত্তি করবে না (যেমন: \"এই চিত্রে দেখা যাচ্ছে...\")\n",
    "                - ব্যক্তিগত মন্তব্য, উপসংহার বা অলংকার ব্যবহার করবে না\n",
    "                - প্রসঙ্গের বাইরে যাবে না\n",
    "\n",
    "                লক্ষ্য: একটি embedding-উপযোগী সারাংশ তৈরি করা যা চিত্রের জ্ঞানকে স্পষ্টভাবে সংরক্ষণ করে।\n",
    "                \"\"\"\n",
    "            }, \n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"{image_url}\"}}\n",
    "        ]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summary_chain = prompt_image_summary | llm_maverick | StrOutputParser()\n",
    "\n",
    "inputs = [{\"image_url\": f\"data:image/jpeg;base64,{img}\"} for img in images]\n",
    "\n",
    "image_summaries = image_summary_chain.batch(inputs, {\"max_concurrency\": 1})\n",
    "\n",
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3411fbe",
   "metadata": {},
   "source": [
    "## Load data and summaries to vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adba41c",
   "metadata": {},
   "source": [
    "**Strategy:**\n",
    "\n",
    "- ChromaDB is used with MultiVectorRetriever and InMemoryDocStore\n",
    "- Jina Embeddings is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dec06eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.embeddings import JinaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdf6f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = JinaEmbeddings(\n",
    "    model_name=\"jina-embeddings-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store for summaries\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"hsc-bangla-book\",\n",
    "    embedding_function=embeddings_model\n",
    ")\n",
    "\n",
    "# storage for parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d9d31",
   "metadata": {},
   "source": [
    "### Load summaries and link to the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72eda43",
   "metadata": {},
   "source": [
    "**Strategy:**\n",
    "\n",
    "1. We will load the text_summaries and image_summaries in vector_store.\n",
    "2. Original texts (Before getting the summary) will be loaded into doc_store and will be linked to text_summaries in vector_store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add text summaries\n",
    "text_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: text_ids[i], \"type\": \"text\"}) \n",
    "    for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(text_ids, texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i], \"type\": \"image\"}) \n",
    "    for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee230baf",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4357de5",
   "metadata": {},
   "source": [
    "### Short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94a16a",
   "metadata": {},
   "source": [
    "Short term memory with max_turns = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortTermMemory:\n",
    "    def __init__(self, max_turns=5):\n",
    "        self.max_turns = max_turns\n",
    "        self.history = []  # Stores (user_input, assistant_output) tuples\n",
    "\n",
    "    def update(self, user_input: str, assistant_output: str):\n",
    "        self.history.append((user_input, assistant_output))\n",
    "        self.history = self.history[-self.max_turns:]\n",
    "\n",
    "    def get_context(self):\n",
    "        return \"\\n\".join([\n",
    "            f\"User: {u}\\nAssistant: {a}\" for u, a in self.history\n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "\n",
    "short_term_memory = ShortTermMemory(max_turns=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcfb80",
   "metadata": {},
   "source": [
    "### Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426d7cf",
   "metadata": {},
   "source": [
    "**Strategy:**\n",
    "\n",
    "1. MultiVectorRetriever is used with similarity search with two options provided:\n",
    "   - Retrieve relevant texts summaries and image summaries from vector_store with raw_texts from doc_store\n",
    "   - Retrive only relevant texts summaries and images summaries\n",
    "2. Retriver will choose top-k (5) options and merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c0f1f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage\n",
    "from base64 import b64decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, use_raw_texts=False, top_k=5):\n",
    "    \"\"\"Retrieve context based on the query using similarity search.\n",
    "    Args:\n",
    "        query (str): The query to search for.\n",
    "        use_raw_texts (bool): Whether to return raw texts or summaries\n",
    "        top_k (int): Number of top documents to retrieve.\n",
    "    \"\"\"\n",
    "    \n",
    "    retrieved_docs = retriever.vectorstore.similarity_search(query, k=top_k)\n",
    "    \n",
    "    final_contexts = []\n",
    "\n",
    "    for doc in retrieved_docs:\n",
    "        doc_type = doc.metadata.get(\"type\")\n",
    "        doc_id = doc.metadata.get(id_key)\n",
    "\n",
    "        if doc_type == \"text\" and use_raw_texts:\n",
    "            # Lookup full raw text from docstore\n",
    "            raw_doc = retriever.docstore.mget([doc_id])[0]\n",
    "            if raw_doc:\n",
    "                final_contexts.append(raw_doc)\n",
    "            else:\n",
    "                # Fallback to summary if raw text not found\n",
    "                final_contexts.append(doc)\n",
    "        else:\n",
    "            # Use the summary directly\n",
    "            final_contexts.append(doc)\n",
    "\n",
    "    return final_contexts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f095f",
   "metadata": {},
   "source": [
    "### Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8d00a",
   "metadata": {},
   "source": [
    "**Strategy:**\n",
    "\n",
    "- Create_chain function will retrieve contexts based on the query and retrieval strategy.\n",
    "- Then will pass it to llm using build_prompt to generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "731166a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(docs):\n",
    "    \"\"\"Parse retrieved documents into text and image content\"\"\"\n",
    "    text_content = \"\"\n",
    "    image_summaries = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_type = doc.metadata.get(\"type\")\n",
    "        if doc_type == \"text\":\n",
    "            text_content += doc.page_content + \"\\n\\n\"\n",
    "        elif doc_type == \"image\":\n",
    "            image_summaries.append(doc.page_content)\n",
    "    \n",
    "    return {\n",
    "        \"text_content\": text_content.strip(),\n",
    "        \"image_summaries\": image_summaries\n",
    "    }\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    context = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    # Combine text content and image summaries\n",
    "    context_text = context[\"text_content\"]\n",
    "    \n",
    "    if context[\"image_summaries\"]:\n",
    "        context_text += \"\\n\\nছবি সংক্রান্ত তথ্য:\\n\"\n",
    "        for i, img_summary in enumerate(context[\"image_summaries\"], 1):\n",
    "            context_text += f\"{i}. {img_summary}\\n\"\n",
    "    \n",
    "    memory_context = short_term_memory.get_context()\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "        তুমি একজন সহায়ক এবং দ্বিভাষিক ভাষা সহকারী, যার কাজ হলো শুধুমাত্র নিচের প্রসঙ্গ ব্যবহার করে ব্যবহারকারীর প্রশ্নের উত্তর দেওয়া। প্রসঙ্গের মধ্যে টেক্সট, টেবিল, ও ছবি থেকে তথ্য থাকতে পারে।\n",
    "\n",
    "        নির্দেশনা:\n",
    "        - নিচের প্রসঙ্গ ব্যবহার করে প্রশ্নের উত্তর দেবে।\n",
    "        - প্রশ্নের উত্তর সংক্ষিপ্ত, স্পষ্ট এবং প্রসঙ্গ-ভিত্তিক হওয়া উচিত।\n",
    "        - কল্পনাপ্রসূত বা অনুমানভিত্তিক তথ্য এড়িয়ে চলবে।\n",
    "        - প্রশ্নের উত্তর দেওয়ার সময় প্রসঙ্গের তথ্য উদ্ধৃত করবে না।\n",
    "        - যদি কোনও প্রশ্নের উত্তর খুঁজে না পাও, তবে {context_text} পড়ে, আউটসাইড নলেজ এ সার্চ করে সঠিক তথ্য খুঁজে, নিজের reasoning ব্যবহার করে বাইরে থেকে সঠিক প্রসঙ্গ খুঁজে উত্তর দেবে।\n",
    "        - কোনো preamble বা অতিরিক্ত ব্যাখ্যা ছাড়া সরাসরি প্রশ্নের উত্তর দেবে এবং Outside knowledge ব্যবহার করলে তা উল্লেখ করে দেবে।\n",
    "\n",
    "        --- প্রসঙ্গ শুরু ---\n",
    "        {context_text}\n",
    "        --- প্রসঙ্গ শেষ ---\n",
    "\n",
    "        প্রশ্ন: {user_question}\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt_template = f\"\"\"\n",
    "    তুমি একজন সহায়ক এবং দ্বিভাষিক ভাষা সহকারী, যার কাজ হলো শুধুমাত্র নিচের প্রসঙ্গ ব্যবহার করে ব্যবহারকারীর প্রশ্নের উত্তর দেওয়া। প্রসঙ্গের মধ্যে টেক্সট, টেবিল, ও ছবি থেকে তথ্য থাকতে পারে।\n",
    "\n",
    "    --- প্রসঙ্গ (Long-Term Memory) ---\n",
    "    {context_text}\n",
    "\n",
    "    --- সাম্প্রতিক কথোপকথন (Short-Term Memory) ---\n",
    "    {memory_context}\n",
    "\n",
    "    প্রশ্ন / Question: {user_question}\n",
    "\n",
    "    Instructions (English for clarity):\n",
    "    - Use only the provided context (Long-Term + Short-Term Memory). Do not invent or assume.\n",
    "    - Respond in the **same language** as the user's question (Bangla or English).\n",
    "    - Keep the response short, clear, and grounded in the provided context.\n",
    "    - Avoid hallucinated, vague, or speculative answers.\n",
    "    - If you cannot find an answer to any question, then read the {context_text} and {memory_context}, search in outside knowledge to find correct information, use your own reasoning to find the correct context from outside and provide the answer.\n",
    "    - Answer the question directly without any preamble or additional explanation, and if you use outside knowledge, mention that.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        HumanMessage(content=prompt_template)\n",
    "    ])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a977bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(use_raw_texts=False, top_k=5):\n",
    "    \"\"\"Create chain with specified retrieval strategy\"\"\"\n",
    "    \n",
    "    def get_context_for_chain(query):\n",
    "        \"\"\"Wrapper function to integrate retrieve_context with the chain\"\"\"\n",
    "        docs = retrieve_context(query, use_raw_texts=use_raw_texts, top_k=top_k)\n",
    "        return parse_docs(docs)\n",
    "    \n",
    "    return (\n",
    "        {\n",
    "            \"context\": RunnableLambda(get_context_for_chain),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(build_prompt) \n",
    "        | llm_scout\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "chain = create_chain(use_raw_texts=False, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-Short Term Memory Example\n",
    "\n",
    "questions = [\n",
    "    \"রবীন্দ্রনাথ ঠাকুরের ছদ্মনাম কি ছিলো?\",\n",
    "    \"কল্যাণীর বাবা কে ছিলেন?\",\n",
    "    \"কাকে অনুপমের ভাগ্য দেবতা বলা হয়েছে?\",\n",
    "    \"ভানুসিংহ ঠাকুর কে ছিলেন?\"\n",
    "]\n",
    "\n",
    "for query in questions:\n",
    "    response = chain.invoke(query)\n",
    "    short_term_memory.update(query, response)\n",
    "    print(f\"Qn: {query}\\nAns: {response}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb93890",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"কল্যাণীর বাবা কে ছিলেন?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2748b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"রবীন্দ্রনাথ ঠাকুরের ছদ্মনাম কি ছিলো?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09e71c",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5530f",
   "metadata": {},
   "source": [
    "For evaluating the RAG system, we may use the RAGAS library.\n",
    "\n",
    "We will evaluate based on :\n",
    "\n",
    "1. Cosine Similarity Score (Similarity between the generated answer and the ground truth) - `Semantic Similarity`\n",
    "2. Groundedness (Is the answer supported by retrieved context?) - `Response Groundness`, `Factual Correctness`, `Faithfulness`\n",
    "3. Relevance (Does the system fetch the most appropriate documents?) - `Context Relevance`, `LLMContextRecall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\n",
    "    \"রবীন্দ্রনাথ ঠাকুরের ছদ্মনাম কি ছিলো?\",\n",
    "    \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\",\n",
    "    \"কল্যাণীর বাবা কে ছিলেন?\",\n",
    "    \"কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\",\n",
    "    \"শম্ভুনাথ সেকরার হাত এ কি পরখ করতে দিয়েছিলেন?\"\n",
    "]\n",
    "\n",
    "expected_responses = [\n",
    "    \"রবীন্দ্রনাথ ঠাকুরের ছদ্মনাম ছিলো ভানুসিংহ ঠাকুর\",\n",
    "    \"অনুপমের ভাষায় শম্ভুনাথ কে সুপুরুষ বলা হয়েছে\",\n",
    "    \"কল্যাণীর বাবা ছিলেন একজন ডাক্তার। তার নাম ছিলো শম্ভুনাথ সেন\",\n",
    "    \"অনুপমের মামাকে তার ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে\",\n",
    "    \"শম্ভুনাথ সেকরার হাত এ একজোড়া ইয়ারিং পরখ করতে দিয়েছিলেন\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(relevant_docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for query, reference in zip(sample_queries, expected_responses):\n",
    "    relevant_docs = retrieve_context(query, top_k=5, use_raw_texts=True)\n",
    "    response = chain.invoke(query)\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": query,\n",
    "            \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
    "            \"response\": response,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "    )\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import SemanticSimilarity, LLMContextRecall, ResponseGroundedness, ContextRelevance, Faithfulness\n",
    "from ragas.metrics._factual_correctness import FactualCorrectness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(llm_scout)\n",
    "evaluator_embedding = LangchainEmbeddingsWrapper(embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022f9a3",
   "metadata": {},
   "source": [
    "### Similarity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd826bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[SemanticSimilarity()],\n",
    "    embeddings=evaluator_embedding,\n",
    "    experiment_name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848cfec",
   "metadata": {},
   "source": [
    "### Groundness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundness_check = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[ResponseGroundedness(), Faithfulness(), FactualCorrectness()],\n",
    "    llm=evaluator_llm,\n",
    "    experiment_name=\"groundness_check\"\n",
    ")\n",
    "\n",
    "groundness_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b5e26",
   "metadata": {},
   "source": [
    "### Relevancy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305eb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_check = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), ContextRelevance()],\n",
    "    llm=evaluator_llm,\n",
    "    experiment_name=\"relevancy_check\"\n",
    ")\n",
    "\n",
    "relevancy_check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
